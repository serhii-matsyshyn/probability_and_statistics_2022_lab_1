---
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

### Serhii Matsyshyn, Sofiia Yamkova, Mykola Yakovkin
### Team 3; 3 mod 5 = 3 the number of data set

## Introduction

During the past three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the *Bayes
theorem*.

One of its applications is **Naive Bayes classifier**, which is a
probabilistic classifier whose aim is to determine which class some
observation probably belongs to by using the Bayes formula:
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong independence assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given
observation. Thus, $\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now
can be calculated as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated from the data as
respective relative frequencies;\
see [this
site](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/)
for more detailed explanations.

## Data description

There are 5 datasets uploaded on the cms.

To determine your variant, take your team number from the list of teams
on cms and take *mod 5* - this is the number of your data set.

-   **0 - authors** This data set consists of citations of three famous
    writers: Edgar Alan Poe, Mary Wollstonecraft Shelley and HP
    Lovecraft. The task with this data set is to classify a piece of
    text with the author who was more likely to write it.

-   **1 - discrimination** This data set consists of tweets that have
    discriminatory (sexism or racism) messages or of tweets that are of
    neutral mood. The task is to determine whether a given tweet has
    discriminatory mood or does not.

-   **2 - fake news** This data set contains data of American news: a
    headline and an abstract of the article. Each piece of news is
    classified as fake or credible. The task is to classify the news
    from test.csv as credible or fake.

-   **3 - sentiment** All the text messages contained in this data set
    are labeled with three sentiments: positive, neutral or negative.
    The task is to classify some text message as the one of positive
    mood, negative or neutral.

-   **4 - spam** This last data set contains SMS messages classified as
    spam or non-spam (ham in the data set). The task is to determine
    whether a given message is spam or non-spam.

Each data set consists of two files: *train.csv* and *test.csv*. The
first one you will need find the probabilities distributions for each of
the features, while the second one is needed for checking how well your
classifier works.

```{r}
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
```

## Instructions

-   The first step is data pre-processing, which includes removing
    punctuation marks and stop words

-   represent each message as a bag-of-words

-   using the training set, calculate all the conditional probabilities
    in formula (1)

-   use those to predict classes for messages in the test set

-   evaluate effectiveness of the classifier by calculating the
    corresponding metrics

-   shortly summarize your work

-   do not forget to submit both the (compiled) Rmd source file and the
    .html output

### Data pre-processing

-   Read the *.csv* data files.
-   Ð¡lear your data from punctuation or other unneeded symbols.
-   Clear you data from stop words. You don't want words as is, and, or
    etc. to affect your probabilities distributions, so it is a wise
    decision to get rid of them. Find list of stop words in the cms
    under the lab task.
-   Represent each test message as its bag-of-words. Here:
    <https://machinelearningmastery.com/gentle-introduction-bag-words-model/>
    you can find general introduction to the bag-of-words model and
    examples on to create it.
-   It is highly recommended to get familiar with R dataframes, it would
    make the work much easier to do.
-   Useful links:
    -   <https://steviep42.github.io/webscraping/book/bagofwords.html#tidytext> -
        example of using *tidytext* to count frequencies of the words.
    -   Basics of Text Mining in R:
        <http://rstudio-pubs-static.s3.amazonaws.com/256588_57b585da6c054349825cba46685d8464.html>
        . Note that it also includes an example on how to create a bag
        of words from your text document.

```{r}
list.files(getwd())
list.files("data/3-sentiment")
```

```{r}
normal_dataset_full_path <- "data/3-sentiment/normal_dataset_full.csv"
stop_words_path <- "data/stop_words.txt"
```

### The provided dataset is split into train and test datasets incorrectly
The dataset is split incorrecltly because:
- The data from test dataset is not connected to the data from train dataset in any way (by content and type of sentences)
- It appears that this data was manually copied from a file, and the data was not pre-mixed.
- The ratio of the number of sentiment sentences is not uniform and not similar to that obtained in the training file.
** Poorly distributed data between the test and training datasets makes neural networks (and any other classifiers used in machine learning) useless **
Therefore, we decided to mix the test and training dataset, and independently make a 70/30 distribution in the program (of course, according to the rules of working with datasets for machine learning - to shuffle the data and avoid the intersection of the test and training dataset, etc.)
Thus, we will get the correct datasets that can be used in this work.

```{r}
# The bad dataset split! DO NOT USE IT!
# test_path <- "data/3-sentiment/test.csv"
# train_path <- "data/3-sentiment/train.csv"
# train <- read.csv(file = train_path, stringsAsFactors = FALSE)
# test <- read.csv(file = test_path, stringsAsFactors = FALSE)
```

```{r}
# The good dataset split! USE IT!
stop_words <- read_file(stop_words_path)
splitted_stop_words <- strsplit(stop_words, split = "\n")
splitted_stop_words <- splitted_stop_words[[1]]

# Read the data from the file
data <- read.csv(file = normal_dataset_full_path, stringsAsFactors = FALSE)
# Shuffle the data

set.seed(999) # make this split reproducible (for the same seed)
shuffled_data <- data[sample(1:nrow(data)), ]

# Split the data into train and test by 70/30
sample <- sample(c(TRUE, FALSE), nrow(shuffled_data), replace = TRUE, prob = c(0.7, 0.3))
train <- shuffled_data[sample, ]
test <- shuffled_data[!sample, ]
```

```{r}
# split sentences into words and remove stop words
tidy_text <- unnest_tokens(train, "splitted", "text", token = "words") %>%
    filter(!splitted %in% splitted_stop_words)

# print(tidy_text)

# This is responsible for adding the missing values to the test set

# for every unique word in the train set
# add this word to each sentiment
unique_words <- unique(tidy_text$splitted)

# create dataframe with unique words and all 3 sentiments
some_df <- data.frame(
    splitted = rep(unique_words, 3),
    sentiment = rep(c("positive", "negative", "neutral"),
        each = length(unique_words)
    )
)

# extend tidy_text by adding some_df
tidy_text <- rbind(tidy_text, some_df)

# count the number of sentiments messages in the train set
sentiments_count <- train %>%
    group_by(sentiment) %>%
    summarise(count = n())

# count the probabilities of each message in the train set
sentiments_prob <- sentiments_count %>%
    mutate(prob = count / sum(count))

# print(sentiments_prob)
```

### Data visualization

Each time you work with some data, you need to understand it before you
start processing it. R has very powerful tools to make nice plots and
visualization. Show what are the most common words for negative and
positive examples as a histogram, word cloud etc. Be creative!

#### TODO: add some plots to visualize the data

## Classifier implementation

```{r}
naiveBayes <- setRefClass("naiveBayes",

    # here it would be wise to have some vars to store intermediate result
    # frequency dict etc. Though pay attention to bag of wards!
    fields = list(
        g_sentiments_prob = "data.frame",
        g_class_prob = "numeric",
        g_total = "data.frame",
        g_final = "data.frame"
    ),
    methods = list(
        # prepare your training data as X - bag of words for each of your
        # messages and corresponding label for the message encoded as 0 or 1
        # (binary classification task)
        fit = function(X, y, sentiments_prob) {
            # create bag of words for each of your messages
            # and corresponding label for the message encoded as 0 or 1

            g_sentiments_prob <<- sentiments_prob
            # print(g_sentiments_prob)

            # count the number of words occurences in each class separately
            final <- X %>% count(splitted, sentiment, sort = TRUE)

            # count total number of words in each class
            total <- final %>%
                group_by(sentiment) %>%
                summarise(total = sum(n))

            # divide the number of words occurences in each class by the total
            # number of words in each class
            final <- final %>%
                left_join(total, by = "sentiment") %>%
                mutate(prob = n / total)

            # calculate the probability of each class and add as new column to Total
            sum_total <- total$total %>% sum()
            class_prob <- total$total / sum_total
            total <- cbind(total, class_prob)

            # make sentiment column the rownames
            rownames(total) <- total$sentiment

            # store the results in the object
            g_class_prob <<- class_prob
            g_total <<- total
            g_final <<- final

            # print(g_final %>% head(1000))
        },

        # return prediction for a single message
        predict = function(message) {
            # split message into words
            splitted_message <- strsplit(message, split = " ")
            splitted_message <- splitted_message[[1]]

            # remove stop words
            splitted_message <- splitted_message[!splitted_message %in% splitted_stop_words]

            # Use sentiments_prob
            g_sentiments_prob_negative_value <- g_sentiments_prob[g_sentiments_prob$sentiment == "negative", "prob"][[1]]
            g_sentiments_prob_positive_value <- g_sentiments_prob[g_sentiments_prob$sentiment == "positive", "prob"][[1]]
            g_sentiments_prob_neutral_value <- g_sentiments_prob[g_sentiments_prob$sentiment == "neutral", "prob"][[1]]

            # print(g_final[g_final$splitted %in% splitted_message & g_final$sentiment == "negative", "prob"])

            prediction_class_negative <- g_sentiments_prob_negative_value * prod(g_final[g_final$splitted %in% splitted_message & g_final$sentiment == "negative", "prob"])
            prediction_class_positive <- g_sentiments_prob_positive_value * prod(g_final[g_final$splitted %in% splitted_message & g_final$sentiment == "positive", "prob"])
            prediction_class_neutral <- g_sentiments_prob_neutral_value * prod(g_final[g_final$splitted %in% splitted_message & g_final$sentiment == "neutral", "prob"])

            # print(prediction_class_negative)
            # print(prediction_class_positive)
            # print(prediction_class_neutral)

            # find the class with the highest probability
            if (prediction_class_negative > prediction_class_positive && prediction_class_negative > prediction_class_neutral) {
                return("negative")
            } else if (prediction_class_positive >= prediction_class_negative && prediction_class_positive >= prediction_class_neutral) {
                return("positive")
            } else {
                return("neutral")
            }
        },

        # score you test set so to get the understanding how well you model
        # works.
        # look at f1 score or precision and recall
        # visualize them
        # try how well your model generalizes to real world data!
        score = function(X_test, y_test) {
            # TODO: visualize the results and use f1 score to evaluate the model
            success_count <- 0
            fail_count <- 0

            for (i in 1:nrow(X_test)) {
                prediction <- predict(X_test[i, "text"])
                if (prediction == y_test[i]) {
                    success_count <- success_count + 1
                } else {
                    # print(c(prediction, y_test[i]))
                    fail_count <- fail_count + 1
                }
            }

            print(success_count)
            print(fail_count)
            print(success_count / (success_count + fail_count))
        }
    )
)

model <- naiveBayes()
model$fit(tidy_text, tidy_text$sentiment, sentiments_prob)
model$predict("s will company")
model$score(test, test$sentiment)
```

## Measure effectiveness of your classifier

-   Note that accuracy is not always a good metric for your classifier.
    Look at precision and recall curves, F1 score metric.
-   Visualize them.
-   Show failure cases.

## Conclusions

Summarize your work by explaining in a few sentences the points listed
below.

-   Describe the method implemented in general. Show what are
    mathematical foundations you are basing your solution on.
-   List pros and cons of the method. This should include the
    limitations of your method, all the assumption you make about the
    nature of your data etc.

# TODO: add some conclusions