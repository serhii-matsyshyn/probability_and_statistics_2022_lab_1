---
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

### Serhii Matsyshyn, Sofiia Yamkova, Mykola Yakovkin
### Team 3; 3 mod 5 = 3 the number of data set

## Introduction

During the past three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the *Bayes
theorem*.

One of its applications is **Naive Bayes classifier**, which is a
probabilistic classifier whose aim is to determine which class some
observation probably belongs to by using the Bayes formula:
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong independence assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given
observation. Thus, $\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now
can be calculated as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated from the data as
respective relative frequencies;\
see [this
site](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/)
for more detailed explanations.

## Data description

There are 5 datasets uploaded on the cms.

To determine your variant, take your team number from the list of teams
on cms and take *mod 5* - this is the number of your data set.

-   **3 - sentiment** All the text messages contained in this data set
    are labeled with three sentiments: positive, neutral or negative.
    The task is to classify some text message as the one of positive
    mood, negative or neutral.  

**! The provided dataset is split into train and test datasets incorrectly (see more information below).**

```{r}
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
```

## Instructions

-   The first step is data pre-processing, which includes removing
    punctuation marks and stop words

-   represent each message as a bag-of-words

-   using the training set, calculate all the conditional probabilities
    in formula (1)

-   use those to predict classes for messages in the test set

-   evaluate effectiveness of the classifier by calculating the
    corresponding metrics

-   shortly summarize your work

-   do not forget to submit both the (compiled) Rmd source file and the
    .html output

### Data pre-processing

-   Read the *.csv* data files.
-   Ð¡lear your data from punctuation or other unneeded symbols.
-   Clear you data from stop words. You don't want words as is, and, or
    etc. to affect your probabilities distributions, so it is a wise
    decision to get rid of them. Find list of stop words in the cms
    under the lab task.
-   Represent each test message as its bag-of-words. Here:
    <https://machinelearningmastery.com/gentle-introduction-bag-words-model/>
    you can find general introduction to the bag-of-words model and
    examples on to create it.
-   It is highly recommended to get familiar with R dataframes, it would
    make the work much easier to do.
-   Useful links:
    -   <https://steviep42.github.io/webscraping/book/bagofwords.html#tidytext> -
        example of using *tidytext* to count frequencies of the words.
    -   Basics of Text Mining in R:
        <http://rstudio-pubs-static.s3.amazonaws.com/256588_57b585da6c054349825cba46685d8464.html>
        . Note that it also includes an example on how to create a bag
        of words from your text document.

```{r}
list.files(getwd())
list.files("data/3-sentiment")
```

```{r}
normal_dataset_full_path <- "data/3-sentiment/normal_dataset_full.csv"
stop_words_path <- "data/stop_words.txt"
```

### The provided dataset is split into train and test datasets incorrectly  
The dataset is split incorrecltly because:  
- The data from test dataset is not connected to the data from train dataset in any way (by content and type of sentences)  
- It appears that this data was manually copied from a file, and the data was not pre-mixed.  
- The ratio of the number of sentiment sentences is not uniform and not similar to that obtained in the training file.  
**Poorly distributed data between the test and training datasets makes neural networks (and any other classifiers used in machine learning) useless**  
Therefore, we decided to mix the test and training dataset, and independently make a 70/30 distribution in the program (of course, according to the rules of working with datasets for machine learning - to shuffle the data and avoid the intersection of the test and training dataset, etc.)  
Thus, we will get the correct datasets that can be used in this work.  

```{r}
# The bad dataset split! DO NOT USE IT!
# test_path <- "data/3-sentiment/test.csv"
# train_path <- "data/3-sentiment/train.csv"
# train <- read.csv(file = train_path, stringsAsFactors = FALSE)
# test <- read.csv(file = test_path, stringsAsFactors = FALSE)
```

```{r}
# The good dataset split! USE IT!
stop_words <- read_file(stop_words_path)
splitted_stop_words <- strsplit(stop_words, split = "\n")
splitted_stop_words <- splitted_stop_words[[1]]

# Read the data from the file
data <- read.csv(file = normal_dataset_full_path, stringsAsFactors = FALSE)
# Shuffle the data

set.seed(999) # make this split reproducible (for the same seed)
shuffled_data <- data[sample(1:nrow(data)), ]

# Split the data into train and test by 70/30
sample <- sample(c(TRUE, FALSE), nrow(shuffled_data), replace = TRUE, prob = c(0.7, 0.3))
train <- shuffled_data[sample, ]
test <- shuffled_data[!sample, ]
```

## Classifier implementation

```{r}
naiveBayes <- setRefClass("naiveBayes",

    # here it would be wise to have some vars to store intermediate result
    # frequency dict etc. Though pay attention to bag of wards!
    fields = list(
        g_sentiments_prob = "data.frame",
        g_class_prob = "numeric",
        g_total = "data.frame",
        g_final = "data.frame"
    ),
    methods = list(
        # prepare your training data as X - bag of words for each of your
        # messages and corresponding label for the message encoded as 0 or 1
        # (binary classification task)

        # Pass X - the messages and Y - the labels (sentiments) to the fit method
        fit = function(X, y, stop_words) {
            # create bag of words for each of your messages
            # and corresponding label for the message (one of three sentiments)

            # split sentences into words and remove stop words
            tidy_text <- unnest_tokens(X, "splitted", "text", token = "words") %>%
                filter(!splitted %in% stop_words)
            # print(tidy_text)

            # This is responsible for adding the missing values to the test set
            # (so that every sentiment has at least one of each words)

            # For every unique word in the train set
            # add this word to each sentiment
            unique_words <- unique(tidy_text$splitted)

            # create dataframe with unique words and all 3 sentiments
            unique_words_sentiments_df <- data.frame(
                splitted = rep(unique_words, 3),
                sentiment = rep(c("positive", "negative", "neutral"),
                    each = length(unique_words)
                )
            )

            # extend tidy_text by adding unique_words_sentiments_df
            tidy_text <- rbind(tidy_text, unique_words_sentiments_df)

            # count the number of sentiments messages in the train set
            sentiments_count <- train %>%
                group_by(sentiment) %>%
                summarise(count = n())

            # count the probabilities of each message in the train set
            sentiments_prob <- sentiments_count %>%
                mutate(prob = count / sum(count))

            # print(sentiments_prob)

            g_sentiments_prob <<- sentiments_prob

            # count the number of words occurences in each class separately
            final <- tidy_text %>% count(splitted, sentiment, sort = TRUE)

            # count total number of words in each class
            total <- final %>%
                group_by(sentiment) %>%
                summarise(total = sum(n))

            # divide the number of words occurences in each class by the total
            # number of words in each class
            final <- final %>%
                left_join(total, by = "sentiment") %>%
                mutate(prob = n / total)

            # calculate the probability of each class and add as new column to Total
            sum_total <- total$total %>% sum()
            class_prob <- total$total / sum_total
            total <- cbind(total, class_prob)

            # make sentiment column the rownames
            rownames(total) <- total$sentiment

            # store the results in the object
            g_class_prob <<- class_prob
            g_total <<- total
            g_final <<- final

            # print(g_final %>% head(1000))
        },

        # return prediction for a single message
        predict = function(message) {
            # split message into words
            splitted_message <- strsplit(message, split = " ")
            splitted_message <- splitted_message[[1]]

            # remove stop words
            splitted_message <- splitted_message[!splitted_message %in% splitted_stop_words]

            # Use sentiments_prob
            g_sentiments_prob_negative_value <- g_sentiments_prob[g_sentiments_prob$sentiment == "negative", "prob"][[1]]
            g_sentiments_prob_positive_value <- g_sentiments_prob[g_sentiments_prob$sentiment == "positive", "prob"][[1]]
            g_sentiments_prob_neutral_value <- g_sentiments_prob[g_sentiments_prob$sentiment == "neutral", "prob"][[1]]

            # print(g_final[g_final$splitted %in% splitted_message & g_final$sentiment == "negative", "prob"])

            prediction_class_negative <- g_sentiments_prob_negative_value * prod(g_final[g_final$splitted %in% splitted_message & g_final$sentiment == "negative", "prob"])
            prediction_class_positive <- g_sentiments_prob_positive_value * prod(g_final[g_final$splitted %in% splitted_message & g_final$sentiment == "positive", "prob"])
            prediction_class_neutral <- g_sentiments_prob_neutral_value * prod(g_final[g_final$splitted %in% splitted_message & g_final$sentiment == "neutral", "prob"])

            # print(prediction_class_negative)
            # print(prediction_class_positive)
            # print(prediction_class_neutral)

            # find the class with the highest probability
            if (prediction_class_negative > prediction_class_positive && prediction_class_negative > prediction_class_neutral) {
                return("negative")
            } else if (prediction_class_positive >= prediction_class_negative && prediction_class_positive >= prediction_class_neutral) {
                return("positive")
            } else {
                return("neutral")
            }
        },

        # score you test set so to get the understanding how well you model
        # works.
        # look at f1 score or precision and recall
        # visualize them
        # try how well your model generalizes to real world data!
        score = function(X_test, y_test) {
            # TODO: visualize the results and use f1 score to evaluate the model
            success_count <- 0
            fail_count <- 0

            for (i in 1:nrow(X_test)) {
                prediction <- predict(X_test[i, "text"])
                if (prediction == y_test[i]) {
                    success_count <- success_count + 1
                } else {
                    # print(c(prediction, y_test[i]))
                    fail_count <- fail_count + 1
                }
            }

            # print(success_count)
            # print(fail_count)
            # print(success_count / (success_count + fail_count))
            return(c(success_count, fail_count))
        }
    )
)

model <- naiveBayes()
model$fit(train, train$sentiment, splitted_stop_words)
score_result <- model$score(test, test$sentiment)
```

### Dataset visualization

Each time you work with some data, you need to understand it before you
start processing it. R has very powerful tools to make nice plots and
visualization. Show what are the most common words for negative and
positive examples as a histogram, word cloud etc. Be creative!

```{r}
# Bar Plot all 3 sentiments (positive, negative, neutral)
train %>%
    count(sentiment) %>%
    ggplot(aes(x = sentiment, y = n)) +
    geom_bar(stat = "identity", fill = "#9f9fff") +
    labs(title = "Bar Plot of Sentiments (positive, negative, neutral)", x = "Sentiment", y = "Count")
```

```{r}
# Bar Plot of top 10 most common words
model$g_final %>%
    head(10) %>%
    ggplot(aes(x = splitted, y = n, fill = sentiment)) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(title = "Top 10 words in all sentiments", x = "Word", y = "Count")
```

## Measure effectiveness of your classifier

-   Note that accuracy is not always a good metric for your classifier.
    Look at precision and recall curves, F1 score metric.
-   Visualize them.
-   Show failure cases.

### Sample prediction
```{r}
# Sample prediction
model$predict("reduce a maximum of 1000 jobs")
```

### Accuracy
```{r}
# Plot success, fail and success rate
score_result_df <- data.frame(
    c("success", "fail"),
    score_result
)

# Plot success and fail count
success <- score_result_df[1, 2]
fail <- score_result_df[2, 2]

ggplot(data = score_result_df, aes(x = score_result_df[, 1], y = score_result_df[, 2])) +
    geom_bar(stat = "identity", fill = "#9f9fff") +
    labs(title = "Success and Fail Count of testing the Classifier", x = "Result", y = "Count")

print("Success rate:")
print(success / (success + fail))
```

### F1 score
```{r}
# F1 Score
score_f1 <- function(X_test, y_test) {
    # positive, negative, neutral
    tp <- c(0, 0, 0)
    fp <- c(0, 0, 0)
    fn <- c(0, 0, 0)

    for (i in 1:nrow(X_test)) {
        prediction <- model$predict(X_test[i, "text"])
        if (prediction == y_test[i]) {
            if (prediction == "positive") {
                tp[1] <- tp[1] + 1
            } else if (prediction == "negative") {
                tp[2] <- tp[2] + 1
            } else {
                tp[3] <- tp[3] + 1
            }
        } else {
            if (prediction == "positive") {
                fp[1] <- fp[1] + 1
            } else if (prediction == "negative") {
                fp[2] <- fp[2] + 1
            } else {
                fp[3] <- fp[3] + 1
            }

            if (y_test[i] == "positive") {
                fn[1] <- fn[1] + 1
            } else if (y_test[i] == "negative") {
                fn[2] <- fn[2] + 1
            } else {
                fn[3] <- fn[3] + 1
            }
        }
    }

    positive_sentiment_f1 <- 2 * tp[1] / (2 * tp[1] + fp[1] + fn[1])
    negative_sentiment_f1 <- 2 * tp[2] / (2 * tp[2] + fp[2] + fn[2])
    neutral_sentiment_f1 <- 2 * tp[3] / (2 * tp[3] + fp[3] + fn[3])

    return(c(positive_sentiment_f1, negative_sentiment_f1, neutral_sentiment_f1))
}

score_f1_result <- score_f1(test, test$sentiment)
print(score_f1_result)

print("F1 Score:")
print(" - For positive sentiment:")
print(score_f1_result[1])
print(" - For negative sentiment:")
print(score_f1_result[2])
print(" - For neutral sentiment:")
print(score_f1_result[3])


# Plot F1 Score using ggplot
ggplot(
    data = data.frame(c("positive", "negative", "neutral"), score_f1_result),
    aes(x = c("positive", "negative", "neutral"), y = score_f1_result)
) +
    geom_bar(stat = "identity", fill = "#59f299") +
    labs(title = "F1 Score of each sentiment", x = "Sentiment", y = "F1 Score")
``` 

### Failure cases
```{r}
# Show 10 random failure cases
test_shuffled <- test[sample(nrow(test)), ]

failed_cases <- data.frame(
    text = character(),
    sentiment = character(),
    prediction = character()
)

for (i in 1:nrow(test_shuffled)) {
    prediction <- model$predict(test_shuffled[i, "text"])
    if (prediction != test_shuffled[i, "sentiment"]) {
        failed_cases <- rbind(failed_cases, data.frame(
            text = test_shuffled[i, "text"],
            sentiment = test_shuffled[i, "sentiment"],
            prediction = prediction
        ))
    }

    if (nrow(failed_cases) == 10) {
        break
    }
}

print("10 random failure cases:")
print(failed_cases)
```

## Conclusions
In this task we had to determine which class each sentence belonged to.  

To do that, we found the probability of each class (number of sentences in the specific class/number of all sentences in data).  
After that we found a probability of each word to appear and having those probabilities, we found the probability of each sentence.  
Now, we should find to which class our sentence belongs.  
Here we used product rule: we multiplied probability of class on probability of sentence, we consider.  
We will have 3 probabilities (cause we have 3 classes) and we should choose the largest one, and that will be class, to which our sentence belongs.  
  
To analyze the dataset, we created a bar plot with top 10 words and where they are repeated and the number of sentences in each class.  
We figured out, that train and test datasets are not split correctly (see information at the top of the notebook), therefore we had to split them again normally.  

Using a Naive Bayes classifier is not very efficient because it does not take word order into account.  
Therefore, the maximum accuracy of the model was 71 percent.  

F1 score shows that there is strong connection between sentiments and the amount of data from input dataset.
That is, the basic rule of creating datasets is violated at the provided dataset: the amount of training data of all classes should be approximately the same.
Otherwise, the model will be biased towards the class with the largest amount of data (in our case, it is neutral sentiment).

To improve the model, we can use a different classifier or use a different dataset.